{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c49405a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import AtariPreprocessing, FrameStackObservation\n",
    "import ale_py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32b5d769",
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.register_envs(ale_py)\n",
    "env = gym.make(\"ALE/VideoPinball-v5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ffcdd37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = AtariPreprocessing(\n",
    "    env,\n",
    "    frame_skip=1,               # Disable frame-skipping as it is set to 4 by default when making the env\n",
    "    screen_size=84,\n",
    "    grayscale_obs=True,\n",
    "    scale_obs=True\n",
    "    )\n",
    "env = FrameStackObservation(env, stack_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be3f22b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box(0.0, 1.0, (4, 84, 84), float32)\n",
      "Action space: Discrete(9)\n",
      "Sample observation: (4, 84, 84)\n"
     ]
    }
   ],
   "source": [
    "obs, _ = env.reset()\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Sample observation: {obs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c1bee067",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNNetwork(nn.Module):\n",
    "    def __init__(self, n_actions):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size=8, stride=4),  # -> (32, 20, 20)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),  # -> (64, 9, 9)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),  # -> (64, 7, 7)\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 7 * 7, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cc2a5ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 20, 20]           8,224\n",
      "              ReLU-2           [-1, 32, 20, 20]               0\n",
      "            Conv2d-3             [-1, 64, 9, 9]          32,832\n",
      "              ReLU-4             [-1, 64, 9, 9]               0\n",
      "            Conv2d-5             [-1, 64, 7, 7]          36,928\n",
      "              ReLU-6             [-1, 64, 7, 7]               0\n",
      "           Flatten-7                 [-1, 3136]               0\n",
      "            Linear-8                  [-1, 512]       1,606,144\n",
      "              ReLU-9                  [-1, 512]               0\n",
      "           Linear-10                    [-1, 9]           4,617\n",
      "================================================================\n",
      "Total params: 1,688,745\n",
      "Trainable params: 1,688,745\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.11\n",
      "Forward/backward pass size (MB): 0.35\n",
      "Params size (MB): 6.44\n",
      "Estimated Total Size (MB): 6.90\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "n_actions = env.action_space.n  # Number of possible actions in the environment\n",
    "model = DQNNetwork(n_actions)\n",
    "model.to(device)\n",
    "summary(model, input_size=obs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "276ba1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=0.00025, amsgrad=True)\n",
    "\n",
    "def q_learning_update(model, state, action, reward, next_state, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Perform a single step of Q-learning update using the Bellman equation.\n",
    "    The optimizer is used to minimize the loss.\n",
    "    \"\"\"\n",
    "    # Convert states to tensors\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    \n",
    "    # Get the Q-values from the model\n",
    "    q_values = model(state_tensor)\n",
    "\n",
    "    # Get the current Q-value for the chosen action\n",
    "    current_q_value = q_values[0][action]\n",
    "\n",
    "    # Compute the target Q-value using the Bellman equation\n",
    "    with torch.no_grad():\n",
    "        next_state_tensor = torch.tensor(next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        next_q_values = model(next_state_tensor)\n",
    "        target_q_value = reward + gamma * next_q_values.max().item()\n",
    "    \n",
    "    \n",
    "    # Compute the loss (Mean Squared Error between current and target Q-values)\n",
    "    target_q_value_tensor = torch.tensor(target_q_value, dtype=torch.float32, device=device)\n",
    "    loss = F.mse_loss(current_q_value, target_q_value_tensor)\n",
    "    \n",
    "    # Backpropagate the loss and update the weights\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccc8c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_update(model, states, actions, rewards, next_states, gamma=0.99, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Perform a Q-learning update using the Bellman equation for a batch of experiences.\n",
    "    \n",
    "    Parameters:\n",
    "        - model: The neural network (DQN) model\n",
    "        - states: Batch of states\n",
    "        - actions: Batch of actions\n",
    "        - rewards: Batch of rewards\n",
    "        - next_states: Batch of next states\n",
    "        - gamma: Discount factor (default 0.99)\n",
    "        - device: The device (\"cuda\" or \"cpu\")\n",
    "    \n",
    "    Returns:\n",
    "        - loss: The loss value\n",
    "    \"\"\"\n",
    "    # Convert data to tensors\n",
    "    states_tensor = torch.tensor(states, dtype=torch.float32, device=device)\n",
    "    next_states_tensor = torch.tensor(next_states, dtype=torch.float32, device=device)\n",
    "    actions_tensor = torch.tensor(actions, dtype=torch.int64, device=device)\n",
    "    rewards_tensor = torch.tensor(rewards, dtype=torch.float32, device=device)\n",
    "\n",
    "    # Get the Q-values for the current states\n",
    "    q_values = model(states_tensor)\n",
    "    \n",
    "    # Select the Q-values corresponding to the actions taken\n",
    "    current_q_values = q_values.gather(1, actions_tensor.view(-1, 1))  # Batch of actions\n",
    "\n",
    "    # Compute the target Q-values using the Bellman equation\n",
    "    with torch.no_grad():\n",
    "        next_q_values = model(next_states_tensor)\n",
    "        target_q_values = rewards_tensor + gamma * next_q_values.max(1)[0]  # max Q-value for next state\n",
    "\n",
    "    # Compute the loss (Mean Squared Error between current Q-values and target Q-values)\n",
    "    loss = F.mse_loss(current_q_values, target_q_values.view(-1, 1))  # Reshape target to match output shape\n",
    "\n",
    "    # Backpropagation and optimization\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0839f51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(env, model, optimizer, episodes=600, gamma=0.99, epsilon=0.1, device=device):\n",
    "\n",
    "    print(f\"Training on device: {device}\")\n",
    "\n",
    "    episode_rewards = []\n",
    "    episode_losses = []\n",
    "\n",
    "    # Ensure the model is on the correct device\n",
    "    model.to(device)\n",
    "    \n",
    "    progress_bar = tqdm(range(episodes), desc=\"Training Progress\", unit=\"episode\", dynamic_ncols=True)\n",
    "    for episode in progress_bar:\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        start_time = time.time()\n",
    "        \n",
    "        while not done:\n",
    "            # Epsilon-greedy action selection\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = np.random.choice(env.action_space.n)\n",
    "            else:\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                action = model(state_tensor).argmax().item()\n",
    "            \n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            # Check if the episode is over due to termination or truncation\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Q-learning update\n",
    "            loss = q_learning_update(model, state, action, reward, next_state, gamma)\n",
    "            \n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "        \n",
    "        episode_time = time.time() - start_time\n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_losses.append(loss.item())\n",
    "\n",
    "    return episode_rewards, episode_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c91932ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb59eb84e0bd4f57a194536fdc01f48f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/600 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Example to run training and visualize results\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m rewards, losses = \u001b[43mtrain_dqn\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mtrain_dqn\u001b[39m\u001b[34m(env, model, optimizer, episodes, gamma, epsilon, device)\u001b[39m\n\u001b[32m     30\u001b[39m done = terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Q-learning update\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m loss = \u001b[43mq_learning_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m total_reward += reward\n\u001b[32m     36\u001b[39m state = next_state\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mq_learning_update\u001b[39m\u001b[34m(model, state, action, reward, next_state, gamma)\u001b[39m\n\u001b[32m     15\u001b[39m current_q_value = q_values[\u001b[32m0\u001b[39m][action]\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Compute the target Q-value using the Bellman equation\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mno_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     19\u001b[39m     next_state_tensor = torch.tensor(next_state, dtype=torch.float32, device=device).unsqueeze(\u001b[32m0\u001b[39m)\n\u001b[32m     20\u001b[39m     next_q_values = model(next_state_tensor)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MSc/CS6482_Deep_RL/Assignments/rl-dqn-atari-videopinball/venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:154\u001b[39m, in \u001b[36m_NoParamDecoratorContextManager.__new__\u001b[39m\u001b[34m(cls, orig_func)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_NoParamDecoratorContextManager\u001b[39;00m(_DecoratorContextManager):\n\u001b[32m    152\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Allow a context manager to be used as a decorator without parentheses.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, orig_func=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    155\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m orig_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    156\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Example to run training and visualize results\n",
    "rewards, losses = train_dqn(env, model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68725cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(rewards)\n",
    "plt.title(\"Episode Rewards\")\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(losses)\n",
    "plt.title(\"Episode Losses\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
